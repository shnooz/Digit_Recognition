{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer using Deep Learning\n",
    "\n",
    "This project is part of the Kaggle competition. You can find more details here: https://www.kaggle.com/c/digit-recognizer#evaluation.\n",
    "But here is an introduction before I dive in:\n",
    "\n",
    "## Goal\n",
    "The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.\n",
    "For every ImageId in the test set, you should predict the correct label. \n",
    "\n",
    "## Dataset used:\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Mar  9 2018, 07:43:39) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data description\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. \n",
    "Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "train = genfromtxt('data/train.csv', delimiter=',')\n",
    "test = genfromtxt('data/test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "First, lets visualize the data (you can change i to see different train images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the image is: (28, 28) \n",
      "The number printed is: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 520\n",
    "img_ex = train[i, 1:].reshape(28,28)\n",
    "print(\"The shape of the image is:\", img_ex.shape, \"\\nThe number printed is:\", int(train[i,0]))\n",
    "imgplot = plt.imshow(img_ex,cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration:\n",
    "Now I would like to explore the data and get some insight about it.\n",
    "first let's see how big are my train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train set is: 42000\n",
      "The size of the test set is: 28000\n"
     ]
    }
   ],
   "source": [
    "print (\"The size of the train set is:\", len(train[1:,:]))\n",
    "print (\"The size of the test set is:\", len(test[1:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to check the train set to see how the different numbers (samples' labels) distribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. ... 7. 6. 9.] The shape of Y_train is: (42000,)\n",
      "The shape of X_train is: (42000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_train = train[1:, 0]\n",
    "X_train = train[1:, 1:]\n",
    "print(Y_train, \"The shape of Y_train is:\", Y_train.shape)\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "plt.hist(Y_train, bins=10)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the data is distributed pretty uniformly. Thats GOOD :)\n",
    "\n",
    "Now I would like to reshape and normalize the train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now X-train shape is: (42000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#The number of sampels in the train set is m:\n",
    "m = X_train.shape[0]\n",
    "#The size of the image is size:\n",
    "size = X_train.shape[1]\n",
    "# h is the height of the image (in our case h=w, since our image is a square):\n",
    "h = int(math.sqrt(size))\n",
    "\n",
    "#Now I reshape the data:\n",
    "X_train = X_train.reshape(m,h,h,1)\n",
    "\n",
    "print(\"Now X-train shape is:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image vectors\n",
    "X_train = X_train/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. ... 7. 6. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10) [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encode Y_train labels to One-hot vectors:\n",
    "labels = Y_train.reshape(-1).astype(int)\n",
    "Y_train = np.eye(10)[labels]\n",
    "print(Y_train.shape, Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to create at this stage a validation set and a train set from the X_train and Y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 28, 28, 1) (33600, 10)\n",
      "(8400, 28, 28, 1) (8400, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LeNet architecture\n",
    "![Figure 2: The LeNet architecture consists of two sets of convolutional, activation, and pooling layers, followed by a fully-connected layer, activation, another fully-connected, and finally a softmax classifier (image source).](images/LeNet.png)\n",
    "\n",
    "\n",
    "The LeNet architecture is an excellent “first architecture” for Convolutional Neural Networks (especially when trained on the MNIST dataset, an image dataset for handwritten digit recognition).\n",
    "LeNet is small and easy to understand — yet large enough to provide interesting results. Furthermore, the combination of LeNet + MNIST is able to run on the CPU, making it easy for beginners to take their first step in Deep Learning and Convolutional Neural Networks.\n",
    "In many ways, LeNet + MNIST is the “Hello, World” equivalent of Deep Learning for image classification.\n",
    "The LeNet architecture consists of the following layers:\n",
    "\n",
    "### LeNet - Convolutional Neural Network in Python\n",
    "\n",
    "INPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC\n",
    "\n",
    "\n",
    "Note: The original LeNet architecture used TANH  activation functions rather than RELU . The reason we use RELU  here is because it tends to give much better classification accuracy due to a number of nice, desirable properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DigitRecognizerModel(input_shape, classes = 10):\n",
    "    \"\"\"\n",
    "    Implementation of the Lenet model for digit recognition.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    #Define the input placeholder as a tensor with shape input_shape.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #First layer of conv=> Relu=> Pool:\n",
    "    # Conv1 layer\n",
    "    X = Conv2D(20, (5,5), name='conv1')(X_input)\n",
    "    #Relu1:\n",
    "    X = Activation('relu')(X)\n",
    "    # MAXPOOL1\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides= (2,2), name='maxpool1')(X)\n",
    "    \n",
    "    #Second layer of conv=> Relu=> Pool:\n",
    "    # Conv2 layer\n",
    "    X = Conv2D(50, (5,5), name='conv2') (X_input)\n",
    "    #Relu2:\n",
    "    X = Activation('relu')(X)\n",
    "    # MAXPOOL2\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides= (2,2), name='maxpool2')(X)\n",
    "    \n",
    "    #Set of FC=> Relu layers:\n",
    "    X = Flatten()(X)\n",
    "    #X = Dense(600)(X)\n",
    "    #X = Activation('relu')(X)\n",
    "    \n",
    "    #Softmax Classifier:\n",
    "    output = Dense(classes, activation='softmax', name='fc')(X)\n",
    "    #output = Activation('softmax', )(X)\n",
    "\n",
    "    # Create model.\n",
    "    model = Model(inputs = X_input, outputs = output, name='DigitRecognizerModel')\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model using the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DigitRecognizerModel = DigitRecognizerModel([28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DigitRecognizerModel.compile(optimizer = \"Adam\", loss = 'categorical_crossentropy', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "33600/33600 [==============================] - 26s 784us/step - loss: 0.5816 - acc: 0.8420\n",
      "Epoch 2/40\n",
      "33600/33600 [==============================] - 27s 806us/step - loss: 0.2052 - acc: 0.9419\n",
      "Epoch 3/40\n",
      "33600/33600 [==============================] - 25s 741us/step - loss: 0.1319 - acc: 0.9632\n",
      "Epoch 4/40\n",
      "33600/33600 [==============================] - 25s 731us/step - loss: 0.0958 - acc: 0.9734\n",
      "Epoch 5/40\n",
      "33600/33600 [==============================] - 26s 761us/step - loss: 0.0782 - acc: 0.9787\n",
      "Epoch 6/40\n",
      "33600/33600 [==============================] - 25s 736us/step - loss: 0.0647 - acc: 0.9823\n",
      "Epoch 7/40\n",
      "33600/33600 [==============================] - 25s 749us/step - loss: 0.0569 - acc: 0.9842\n",
      "Epoch 8/40\n",
      "33600/33600 [==============================] - 25s 754us/step - loss: 0.0518 - acc: 0.9858\n",
      "Epoch 9/40\n",
      "33600/33600 [==============================] - 25s 754us/step - loss: 0.0468 - acc: 0.9866\n",
      "Epoch 10/40\n",
      "33600/33600 [==============================] - 25s 754us/step - loss: 0.0421 - acc: 0.9881\n",
      "Epoch 11/40\n",
      "33600/33600 [==============================] - 25s 756us/step - loss: 0.0380 - acc: 0.9895\n",
      "Epoch 12/40\n",
      "33600/33600 [==============================] - 25s 756us/step - loss: 0.0350 - acc: 0.9901\n",
      "Epoch 13/40\n",
      "33600/33600 [==============================] - 25s 752us/step - loss: 0.0314 - acc: 0.9913\n",
      "Epoch 14/40\n",
      "33600/33600 [==============================] - 25s 752us/step - loss: 0.0295 - acc: 0.9920\n",
      "Epoch 15/40\n",
      "33600/33600 [==============================] - 25s 758us/step - loss: 0.0265 - acc: 0.9926\n",
      "Epoch 16/40\n",
      "33600/33600 [==============================] - 25s 750us/step - loss: 0.0258 - acc: 0.9930\n",
      "Epoch 17/40\n",
      "33600/33600 [==============================] - 25s 738us/step - loss: 0.0241 - acc: 0.9929\n",
      "Epoch 18/40\n",
      "33600/33600 [==============================] - 25s 746us/step - loss: 0.0220 - acc: 0.9942\n",
      "Epoch 19/40\n",
      "33600/33600 [==============================] - 25s 745us/step - loss: 0.0212 - acc: 0.9942\n",
      "Epoch 20/40\n",
      "33600/33600 [==============================] - 25s 751us/step - loss: 0.0194 - acc: 0.9949\n",
      "Epoch 21/40\n",
      "33600/33600 [==============================] - 25s 748us/step - loss: 0.0171 - acc: 0.9959\n",
      "Epoch 22/40\n",
      "33600/33600 [==============================] - 24s 726us/step - loss: 0.0165 - acc: 0.9959\n",
      "Epoch 23/40\n",
      "33600/33600 [==============================] - 25s 755us/step - loss: 0.0153 - acc: 0.9964\n",
      "Epoch 24/40\n",
      "33600/33600 [==============================] - 24s 718us/step - loss: 0.0139 - acc: 0.9971\n",
      "Epoch 25/40\n",
      "33600/33600 [==============================] - 24s 726us/step - loss: 0.0139 - acc: 0.9968\n",
      "Epoch 26/40\n",
      "33600/33600 [==============================] - 25s 749us/step - loss: 0.0128 - acc: 0.9972\n",
      "Epoch 27/40\n",
      "33600/33600 [==============================] - 24s 729us/step - loss: 0.0111 - acc: 0.9981\n",
      "Epoch 28/40\n",
      "33600/33600 [==============================] - 24s 712us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 29/40\n",
      "33600/33600 [==============================] - 24s 716us/step - loss: 0.0098 - acc: 0.9984\n",
      "Epoch 30/40\n",
      "33600/33600 [==============================] - 24s 728us/step - loss: 0.0093 - acc: 0.9981\n",
      "Epoch 31/40\n",
      "33600/33600 [==============================] - 24s 728us/step - loss: 0.0087 - acc: 0.9985\n",
      "Epoch 32/40\n",
      "33600/33600 [==============================] - 25s 729us/step - loss: 0.0079 - acc: 0.9988\n",
      "Epoch 33/40\n",
      "33600/33600 [==============================] - 24s 724us/step - loss: 0.0073 - acc: 0.9988\n",
      "Epoch 34/40\n",
      "33600/33600 [==============================] - 25s 743us/step - loss: 0.0068 - acc: 0.9993\n",
      "Epoch 35/40\n",
      "33600/33600 [==============================] - 25s 744us/step - loss: 0.0071 - acc: 0.9988\n",
      "Epoch 36/40\n",
      "33600/33600 [==============================] - 25s 740us/step - loss: 0.0059 - acc: 0.9993\n",
      "Epoch 37/40\n",
      "33600/33600 [==============================] - 24s 729us/step - loss: 0.0061 - acc: 0.9991\n",
      "Epoch 38/40\n",
      "33600/33600 [==============================] - 25s 732us/step - loss: 0.0055 - acc: 0.9995\n",
      "Epoch 39/40\n",
      "33600/33600 [==============================] - 25s 749us/step - loss: 0.0049 - acc: 0.9994\n",
      "Epoch 40/40\n",
      "33600/33600 [==============================] - 25s 739us/step - loss: 0.0047 - acc: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27f9a0e4a20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DigitRecognizerModel.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400/8400 [==============================] - 2s 284us/step\n",
      "\n",
      "Loss = 0.06130497150397527\n",
      "Test Accuracy = 0.9840476190476191\n"
     ]
    }
   ],
   "source": [
    "preds = DigitRecognizerModel.evaluate(x = X_val, y = Y_val)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the test set:\n",
    "That's pretty good! Now let's generate the submission file (predictions on the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now X-train shape is: (28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test = test[1:,:]\n",
    "\n",
    "#The number of sampels in the train set is m:\n",
    "m = X_test.shape[0]\n",
    "#The size of the image is size:\n",
    "size = X_test.shape[1]\n",
    "# h is the height of the image (in our case h=w, since our image is a square):\n",
    "h = int(math.sqrt(size))\n",
    "\n",
    "#Now I reshape the data:\n",
    "X_test = X_test.reshape(m,h,h,1)\n",
    "\n",
    "print(\"Now X-train shape is:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image vectors\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Predict results\n",
    "test_preds = DigitRecognizerModel.predict(x = X_test)\n",
    "print(test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the index with the maximum probability:\n",
    "test_labels = np.argmax(test_preds, axis=1)\n",
    "test_labels = pd.Series(test_labels, name='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), test_labels], axis = 1)\n",
    "submission.to_csv(\"cnn_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
